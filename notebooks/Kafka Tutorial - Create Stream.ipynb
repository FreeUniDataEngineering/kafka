{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Kafka Producer and Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "\n",
    "def connect_kafka_producer(server):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[server], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'broker:9093'\n",
    "\n",
    "heroes_topic = 'heroes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=server, \n",
    "    client_id='Franz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='heroes', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Topic\n",
    "topic_list = [NewTopic(name=heroes_topic, num_partitions=3, replication_factor=1)]\n",
    "\n",
    "admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='heroes', error_code=0)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Topic\n",
    "# admin_client.delete_topics(['heroes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming Data to Kafka Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAVA_HOME for Spark\n",
    "import os\n",
    "\n",
    "spark_version = '3.2.0'\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"jdk-11\"\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-10455bae-dd1e-4626-bc10-b7420da55ec7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 5375ms :: artifacts dl 991ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-10455bae-dd1e-4626-bc10-b7420da55ec7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/345ms)\n",
      "21/12/30 16:54:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/30 16:54:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .appName('creating stream') \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0')\\\n",
    "      .getOrCreate()\\\n",
    "\n",
    "sc = spark._sc\n",
    "\n",
    "# spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stream from specific directory\n",
    "\n",
    "stream_dir = 'individual_heroes'\n",
    "\n",
    "schema = StructType([StructField('Name',StringType(),True),\n",
    "                     StructField('Alignment',StringType(),True),\n",
    "                     StructField('Intelligence',DecimalType(10, 2),True),\n",
    "                     StructField('Strength',DecimalType(10, 2),True),\n",
    "                     StructField('Speed',IntegerType(),True),\n",
    "                     StructField('Durability',DecimalType(10, 2),True),\n",
    "                     StructField('Power',DecimalType(10, 2),True),\n",
    "                     StructField('Combat',DecimalType(10, 2),True),\n",
    "                     StructField('Total',DecimalType(10, 2),True),\n",
    "                     StructField('Id',IntegerType(),True)])\n",
    "\n",
    "df = spark.readStream\\\n",
    "  .schema(schema)\\\n",
    "  .csv(stream_dir)\\\n",
    "\n",
    "\n",
    "# To KVs\n",
    "df = df\\\n",
    "    .withColumn('Value', f.to_json(f.struct(*[f.col('Name').alias('Name'), \n",
    "                                    f.col('Alignment').alias('Alignment'), \n",
    "                                    f.col('Intelligence').alias('Intelligence'), \n",
    "                                    f.col('Strength').alias('Strength'), \n",
    "                                    f.col('Speed').alias('Speed'), \n",
    "                                    f.col('Durability').alias('Durability'), \n",
    "                                    f.col('Power').alias('Power'),\n",
    "                                    f.col('Combat').alias('Combat'),\n",
    "                                    f.col('Total').alias('Total')])))\\\n",
    "    .selectExpr(\"CAST(Id AS STRING) as key\", \"CAST(Value AS STRING) as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/30 16:59:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "21/12/30 16:59:36 WARN FileStreamSource: Listed 370 file(s) in 2803 ms\n",
      "21/12/30 17:00:10 WARN FileStreamSource: Listed 377 file(s) in 2613 ms          \n",
      "21/12/30 17:00:19 WARN FileStreamSource: Listed 379 file(s) in 3019 ms\n",
      "21/12/30 17:00:28 WARN FileStreamSource: Listed 380 file(s) in 2626 ms\n",
      "21/12/30 17:00:38 WARN FileStreamSource: Listed 382 file(s) in 3874 ms\n",
      "21/12/30 17:00:47 WARN FileStreamSource: Listed 384 file(s) in 2582 ms\n",
      "21/12/30 17:00:54 WARN FileStreamSource: Listed 386 file(s) in 2559 ms\n",
      "21/12/30 17:01:04 WARN FileStreamSource: Listed 388 file(s) in 2587 ms\n",
      "21/12/30 17:01:13 WARN FileStreamSource: Listed 389 file(s) in 2628 ms\n",
      "21/12/30 17:01:22 WARN FileStreamSource: Listed 391 file(s) in 3426 ms\n",
      "21/12/30 17:01:32 WARN FileStreamSource: Listed 393 file(s) in 2426 ms\n",
      "21/12/30 17:01:42 WARN FileStreamSource: Listed 395 file(s) in 3766 ms\n",
      "21/12/30 17:01:52 WARN FileStreamSource: Listed 397 file(s) in 2895 ms\n",
      "21/12/30 17:02:01 WARN FileStreamSource: Listed 399 file(s) in 2865 ms\n",
      "21/12/30 17:02:11 WARN FileStreamSource: Listed 401 file(s) in 2525 ms\n",
      "21/12/30 17:02:19 WARN FileStreamSource: Listed 403 file(s) in 2862 ms\n",
      "21/12/30 17:02:26 WARN FileStreamSource: Listed 404 file(s) in 2133 ms\n",
      "21/12/30 17:02:36 WARN FileStreamSource: Listed 406 file(s) in 3529 ms\n",
      "21/12/30 17:02:46 WARN FileStreamSource: Listed 408 file(s) in 3329 ms\n",
      "21/12/30 17:02:57 WARN FileStreamSource: Listed 410 file(s) in 3497 ms\n",
      "21/12/30 17:03:07 WARN FileStreamSource: Listed 412 file(s) in 4110 ms\n",
      "21/12/30 17:03:15 WARN FileStreamSource: Listed 414 file(s) in 2888 ms\n",
      "21/12/30 17:03:25 WARN FileStreamSource: Listed 416 file(s) in 3082 ms\n",
      "21/12/30 17:03:37 WARN FileStreamSource: Listed 418 file(s) in 4666 ms\n",
      "21/12/30 17:03:47 WARN FileStreamSource: Listed 420 file(s) in 3282 ms\n",
      "21/12/30 17:03:58 WARN FileStreamSource: Listed 422 file(s) in 3613 ms\n",
      "21/12/30 17:04:10 WARN FileStreamSource: Listed 424 file(s) in 3681 ms\n",
      "21/12/30 17:04:20 WARN FileStreamSource: Listed 426 file(s) in 3433 ms\n",
      "21/12/30 17:04:34 WARN FileStreamSource: Listed 429 file(s) in 5111 ms\n",
      "21/12/30 17:04:44 WARN FileStreamSource: Listed 431 file(s) in 3651 ms\n",
      "21/12/30 17:04:53 WARN FileStreamSource: Listed 433 file(s) in 4198 ms\n",
      "21/12/30 17:05:02 WARN FileStreamSource: Listed 435 file(s) in 3862 ms\n",
      "21/12/30 17:05:11 WARN FileStreamSource: Listed 437 file(s) in 3322 ms\n",
      "21/12/30 17:05:21 WARN FileStreamSource: Listed 439 file(s) in 3014 ms\n",
      "21/12/30 17:05:30 WARN FileStreamSource: Listed 441 file(s) in 3182 ms\n",
      "21/12/30 17:05:39 WARN FileStreamSource: Listed 442 file(s) in 3380 ms\n",
      "21/12/30 17:05:49 WARN FileStreamSource: Listed 444 file(s) in 3243 ms\n",
      "21/12/30 17:05:59 WARN FileStreamSource: Listed 446 file(s) in 3682 ms\n",
      "21/12/30 17:06:09 WARN FileStreamSource: Listed 448 file(s) in 3531 ms\n",
      "21/12/30 17:06:19 WARN FileStreamSource: Listed 450 file(s) in 3503 ms\n",
      "21/12/30 17:06:26 WARN FileStreamSource: Listed 452 file(s) in 2791 ms\n",
      "21/12/30 17:06:34 WARN FileStreamSource: Listed 453 file(s) in 2858 ms\n",
      "21/12/30 17:06:41 WARN FileStreamSource: Listed 455 file(s) in 2589 ms\n",
      "21/12/30 17:06:48 WARN FileStreamSource: Listed 456 file(s) in 2595 ms\n",
      "21/12/30 17:06:56 WARN FileStreamSource: Listed 458 file(s) in 3165 ms\n",
      "21/12/30 17:07:05 WARN FileStreamSource: Listed 460 file(s) in 3309 ms\n",
      "21/12/30 17:07:13 WARN FileStreamSource: Listed 461 file(s) in 3076 ms\n",
      "21/12/30 17:07:22 WARN FileStreamSource: Listed 463 file(s) in 3356 ms\n",
      "21/12/30 17:07:32 WARN FileStreamSource: Listed 465 file(s) in 3630 ms\n",
      "21/12/30 17:07:42 WARN FileStreamSource: Listed 467 file(s) in 4129 ms\n",
      "21/12/30 17:07:52 WARN FileStreamSource: Listed 469 file(s) in 3387 ms\n",
      "21/12/30 17:08:04 WARN FileStreamSource: Listed 471 file(s) in 3456 ms\n",
      "21/12/30 17:08:16 WARN FileStreamSource: Listed 474 file(s) in 3362 ms\n",
      "21/12/30 17:08:26 WARN FileStreamSource: Listed 476 file(s) in 3338 ms\n",
      "21/12/30 17:08:35 WARN FileStreamSource: Listed 477 file(s) in 3501 ms\n",
      "21/12/30 17:08:45 WARN FileStreamSource: Listed 479 file(s) in 3745 ms\n",
      "21/12/30 17:08:55 WARN FileStreamSource: Listed 481 file(s) in 4890 ms\n",
      "21/12/30 17:09:06 WARN FileStreamSource: Listed 483 file(s) in 3801 ms\n",
      "21/12/30 17:09:17 WARN FileStreamSource: Listed 486 file(s) in 3375 ms\n",
      "21/12/30 17:09:27 WARN FileStreamSource: Listed 488 file(s) in 4267 ms\n",
      "21/12/30 17:09:40 WARN FileStreamSource: Listed 490 file(s) in 3667 ms\n",
      "21/12/30 17:09:48 WARN FileStreamSource: Listed 492 file(s) in 3135 ms\n",
      "21/12/30 17:09:56 WARN FileStreamSource: Listed 494 file(s) in 2865 ms\n",
      "21/12/30 17:10:04 WARN FileStreamSource: Listed 495 file(s) in 3256 ms\n",
      "21/12/30 17:10:12 WARN FileStreamSource: Listed 497 file(s) in 2943 ms\n",
      "21/12/30 17:10:21 WARN FileStreamSource: Listed 499 file(s) in 3442 ms\n",
      "21/12/30 17:10:30 WARN FileStreamSource: Listed 500 file(s) in 3806 ms\n",
      "21/12/30 17:10:40 WARN FileStreamSource: Listed 502 file(s) in 3772 ms\n",
      "21/12/30 17:10:49 WARN FileStreamSource: Listed 504 file(s) in 3563 ms\n",
      "21/12/30 17:10:59 WARN FileStreamSource: Listed 506 file(s) in 4350 ms\n",
      "21/12/30 17:11:07 WARN FileStreamSource: Listed 508 file(s) in 3604 ms\n",
      "21/12/30 17:11:17 WARN FileStreamSource: Listed 510 file(s) in 3828 ms\n",
      "21/12/30 17:11:27 WARN FileStreamSource: Listed 512 file(s) in 3582 ms\n",
      "21/12/30 17:11:35 WARN FileStreamSource: Listed 513 file(s) in 3362 ms\n",
      "21/12/30 17:11:44 WARN FileStreamSource: Listed 515 file(s) in 3448 ms\n",
      "21/12/30 17:11:54 WARN FileStreamSource: Listed 517 file(s) in 4574 ms\n",
      "21/12/30 17:12:04 WARN FileStreamSource: Listed 519 file(s) in 3640 ms\n",
      "21/12/30 17:12:16 WARN FileStreamSource: Listed 521 file(s) in 4603 ms\n",
      "21/12/30 17:12:26 WARN FileStreamSource: Listed 523 file(s) in 3777 ms\n",
      "21/12/30 17:12:35 WARN FileStreamSource: Listed 525 file(s) in 3297 ms\n",
      "21/12/30 17:12:45 WARN FileStreamSource: Listed 527 file(s) in 4391 ms\n",
      "21/12/30 17:12:53 WARN FileStreamSource: Listed 529 file(s) in 3303 ms\n",
      "21/12/30 17:13:01 WARN FileStreamSource: Listed 531 file(s) in 3635 ms\n",
      "21/12/30 17:13:11 WARN FileStreamSource: Listed 532 file(s) in 3882 ms\n",
      "21/12/30 17:13:19 WARN FileStreamSource: Listed 534 file(s) in 3425 ms\n",
      "21/12/30 17:13:28 WARN FileStreamSource: Listed 536 file(s) in 3580 ms\n",
      "21/12/30 17:13:37 WARN FileStreamSource: Listed 538 file(s) in 3170 ms\n",
      "21/12/30 17:13:45 WARN FileStreamSource: Listed 539 file(s) in 3388 ms\n",
      "21/12/30 17:13:53 WARN FileStreamSource: Listed 541 file(s) in 3042 ms\n",
      "21/12/30 17:14:01 WARN FileStreamSource: Listed 543 file(s) in 3728 ms\n",
      "21/12/30 17:14:10 WARN FileStreamSource: Listed 544 file(s) in 4066 ms\n",
      "21/12/30 17:14:19 WARN FileStreamSource: Listed 546 file(s) in 3710 ms\n",
      "21/12/30 17:14:29 WARN FileStreamSource: Listed 548 file(s) in 3634 ms\n",
      "21/12/30 17:14:37 WARN FileStreamSource: Listed 550 file(s) in 2966 ms\n",
      "21/12/30 17:14:45 WARN FileStreamSource: Listed 551 file(s) in 3643 ms\n",
      "21/12/30 17:14:53 WARN FileStreamSource: Listed 553 file(s) in 3007 ms\n",
      "21/12/30 17:15:00 WARN FileStreamSource: Listed 554 file(s) in 3281 ms\n",
      "21/12/30 17:15:08 WARN FileStreamSource: Listed 556 file(s) in 2961 ms\n",
      "21/12/30 17:15:16 WARN FileStreamSource: Listed 558 file(s) in 3086 ms\n",
      "21/12/30 17:15:24 WARN FileStreamSource: Listed 559 file(s) in 3752 ms\n",
      "21/12/30 17:15:32 WARN FileStreamSource: Listed 561 file(s) in 3517 ms\n",
      "21/12/30 17:15:40 WARN FileStreamSource: Listed 562 file(s) in 3152 ms\n",
      "21/12/30 17:15:48 WARN FileStreamSource: Listed 564 file(s) in 3526 ms\n",
      "21/12/30 17:15:58 WARN FileStreamSource: Listed 566 file(s) in 4572 ms\n"
     ]
    }
   ],
   "source": [
    "# To Kafka\n",
    "\n",
    "df \\\n",
    "   .writeStream \\\n",
    "   .format(\"kafka\") \\\n",
    "   .option(\"kafka.bootstrap.servers\", \"PLAINTEXT://broker:9093\") \\\n",
    "   .option(\"topic\", heroes_topic) \\\n",
    "   .option(\"checkpointLocation\", \"checkpoint\")\\\n",
    "   .start()\\\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
